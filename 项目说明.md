# 从零开始：用大模型打造咖啡机械臂

## 作者介绍

**李昊洋** - 中国计量大学 23级人工智能3班

- **GitHub**: [231055558](https://github.com/231055558)
- 欢迎访问我的 GitHub 主页

- **网盘**: [点击下载代码](https://pan.baidu.com/s/1LDwK0IT1q1XgwomQq5daOw?pwd=lhyn)
- 提取码: lhyn 

## 项目起因

这个项目源于我导师杨力老师的一次邀请——为本科新生做一场前沿技术讲习。要求是：**偏实操、不要太难、大家都能复现**。

作为一个经常因为不好意思点咖啡提各种要求而困扰的人，借此机会我想做一个能帮我定制咖啡的智能机器人。但现有的具身智能方案（模仿学习、端到端VLA）对本科新生来说门槛太高，成本也不低。

于是我给自己定了几个硬性约束：
1. **2小时内可复现** - 从环境搭建到运行成功
2. **零成本** - 纯软件仿真，不需要真实机器人，大模型调用api
3. **代码简洁** - 每个函数都尽可能短，方便理解
4. **技术前沿但不复杂** - 用 LLM + VLM + PyBullet，让学生体验 AI 与机器人的结合

这个项目的核心理念是：**在没有现成框架的情况下，如何快速验证自己的想法**。这才是一个 DIY 代码工程师应该具备的能力。

---

## 技术选型

### 为什么选择这套技术栈？

| 技术 | 作用 | 优势 |
|------|------|------|
| **PyBullet** | 物理仿真引擎 | 免费、轻量、API 简单，无需真实硬件 |
| **大语言模型 (LLM)** | 自然语言理解 + 逻辑推理 | 理解订单、生成配方、规划动作序列 |
| **视觉语言模型 (VLM)** | 图像理解 | 识别场景中的物体位置 |
| **智谱 AI** | API 提供商 | 国内访问稳定，免费 |

### 系统架构

```
用户订单（自然语言）
    ↓
订单理解（LLM）→ 生成咖啡配方
    ↓
场景拍摄（虚拟相机）→ 获取当前画面
    ↓
视觉识别（VLM）→ 识别原料位置
    ↓
动作规划（LLM）→ 生成机械臂轨迹
    ↓
物理执行（PyBullet + IK）→ 控制机械臂动作
```

整个流程就是一个端到端的具身智能系统：**从语言到视觉，从规划到执行**。

---

## 系统模块详解

### 1. 仿真环境：CoffeeShopServer (`coffee_env.py`)

**作用**：搭建一个虚拟的咖啡厅场景

**核心功能**：
- `__init__()` - 初始化 PyBullet 仿真环境，创建场景
- `_create_scene()` - 创建吧台、3层货架、9个原料瓶、咖啡杯和机械臂
- `_create_camera()` - 放置虚拟相机（用于后续拍照）
- `reset_scene()` - 重置所有物体和机械臂到初始位置
- `swap_bottles()` - 交换两个瓶子的位置（用于测试视觉识别的鲁棒性）
- `run()` - 运行物理仿真循环（240Hz）

**设计亮点**：
- 9个原料瓶用不同颜色区分（深棕色咖啡、蓝色水、白色牛奶等）
- 使用多线程监听键盘输入，可以在仿真运行时重置场景或交换瓶子
- 货架采用阶梯设计，3行3列，便于视觉识别

---

### 2. 虚拟相机：CameraManager (`camera_manager.py`)

**作用**：在仿真环境中拍摄场景照片

**核心功能**：
- `__init__()` - 设置相机参数（位置、朝向、FOV等）
- `capture_image()` - 拍摄 640x640 的 RGB 图像并保存

**技术细节**：
- 相机位置：`[0, -0.5, 1.3]`，从机械臂后上方俯拍
- 使用 PyBullet 的 `getCameraImage()` API 获取渲染图像
- 输出标准 PNG 格式，便于后续 VLM 处理

---

### 3. 配方生成：RecipeLLM (`recipe_llm.py`)

**作用**：将用户的自然语言订单转化为结构化配方

**核心功能**：
- `__init__()` - 初始化智谱 AI 客户端
- `generate_recipe()` - 接收订单，返回 JSON 格式的配方

**Prompt 设计思路**：
1. **库存约束**：明确告知 LLM 只有 9 种原料
2. **配方参考**：提供标准配方（美式、拿铁、摩卡等）
3. **逻辑规则**：
   - 容量控制（默认 350ml，超过 1000ml 拒绝）
   - 原料顺序（固体 → 浓缩 → 糖浆 → 主液）
   - 拒绝逻辑（缺料或离谱需求时拒绝）
4. **输出格式**：强制 JSON 输出，包含成功/拒绝状态

**示例输入/输出**：
```
输入："来一杯热拿铁"
输出：
{
  "status": "success",
  "product_name": "热拿铁",
  "total_volume_ml": 350,
  "steps": [
    {"ingredient": "ESPRESSO", "amount_ml": 40},
    {"ingredient": "MILK", "amount_ml": 310}
  ]
}
```

---

### 4. 视觉识别：VisionLLM (`vision_llm.py`)

**作用**：识别场景中每个原料瓶的位置

**核心功能**：
- `__init__()` - 初始化智谱 AI 视觉模型客户端
- `_encode_image()` - 将图片编码为 Base64 格式
- `detect_ingredients()` - 识别图像，返回原料位置映射

**Prompt 设计思路**：
1. **颜色特征库**：为每种原料定义颜色描述（如"ESPRESSO 是深黑褐色"）
2. **空间描述**：告知 VLM 货架是 3行3列，定义行列索引规则
3. **输出要求**：返回 `{"原料名": [row, col]}` 的 JSON 格式

**示例输出**：
```json
{
  "ESPRESSO": [0, 0],
  "WATER": [0, 1],
  "MILK": [0, 2],
  "VANILLA": [1, 0],
  ...
}
```

**技术亮点**：
- 使用 VLM 而非传统 CV 算法，无需训练数据集
- 通过颜色描述实现"零样本"物体识别

---

### 5. 运动规划：End2EndPlanner (`llm_planner_end2end.py`)

**作用**：为每个原料生成完整的机械臂动作序列

**核心功能**：
- `__init__()` - 初始化 LLM 客户端
- `plan_ingredient()` - 为单个原料生成抓取-倒水-放回的完整动作
- `plan_recipe()` - 遍历配方，生成所有原料的动作序列
- `_clean_json()` - 清理 LLM 输出的 Markdown 代码块

**Prompt 设计思路**：
1. **坐标计算公式**：教 LLM 如何根据 `[row, col]` 计算实际坐标
   - `Target_X = (col - 1) * 0.2`
   - `Target_Z = 0.8 + (row * 0.15)`
2. **标准操作流程（SOP）**：14步固定流程
   - 移动到工作点 → 靠近瓶子 → 抓取 → 后退 → 移动到杯子 → 旋转倒水 → 回正 → 放回
3. **安全约束**：严禁垂直提起，必须水平移动（避免碰撞）

**示例输出**：
```json
[
  {"cmd": "MOVE", "pos": [0, -0.2, 1.0]},      // 移动到工作点
  {"cmd": "MOVE", "pos": [0.2, -0.05, 0.8]},   // 靠近瓶子
  {"cmd": "MOVE", "pos": [0.2, 0.09, 0.8]},    // 前伸到抓取点
  {"cmd": "GRAB", "width": 0.0},                // 闭合夹爪
  ...
]
```

---

### 6. 机械臂控制：RobotController (`robot_controller.py`)

**作用**：执行底层的 IK 计算和关节控制

**核心功能**：
- `__init__()` - 连接到 PyBullet 仿真服务器
- `_find_robot_id()` - 查找 Franka Panda 机械臂的 ID
- `get_current_joint_angles()` - 获取当前关节角度
- `move_to_smooth()` - 平滑移动到目标位置（带 IK 计算）
- `grab()` - 控制夹爪开合
- `rotate_wrist()` - 旋转手腕（用于倒水）

**技术细节**：
- **逆运动学（IK）**：输入目标位置 `[x, y, z]`，输出7个关节角度
- **零空间约束**：限制关节运动范围，保持舒适姿态
- **平滑插值**：在起点和终点之间进行线性插值，避免突变
- **高精度参数**：
  - `maxNumIterations=100` - IK 迭代次数
  - `residualThreshold=1e-5` - 误差阈值

---

### 7. 主控制器：CoffeeAgent (`agent.py`)

**作用**：串联所有模块，完成端到端流程

**核心功能**：
- `__init__()` - 初始化所有子系统（相机、控制器、3个 AI 大脑）
- `run()` - 主循环，接收用户输入
- `_process_order()` - 处理单个订单的完整流程
- `_execute_physical_actions()` - 执行动作序列（MOVE/GRAB/WRIST/WAIT）

**执行流程**：
```
[1/4] 订单理解 → 生成配方
[2/4] 视觉扫描 → 识别位置 + 核对库存
[3/4] 动作规划 → 生成轨迹
[4/4] 物理执行 → 控制机械臂
```

**设计亮点**：
- 模块化设计，每个"大脑"独立运行
- 库存核对逻辑：如果配方需要的原料不在视觉地图中，则拒绝制作
- 错误处理：任何环节失败都会中断流程并提示用户

---

## 项目实现过程

### 第一步：搭建仿真环境

从零开始创建 `coffee_env.py`：
1. 创建一个平面（地面）
2. 创建一个桌子（吧台）
3. 用 for 循环生成 3 层货架
4. 用不同颜色的方块代表 9 种原料
5. 加载 Franka Panda 机械臂模型
6. 添加键盘监听功能（重置场景、交换瓶子）

---

### 第二步：实现相机拍照

创建 `camera_manager.py`，调用 PyBullet 的相机 API：
```python
p.getCameraImage(width=640, height=640, ...)
```

---

### 第三步：设计 Prompt，调用 LLM/VLM

这是最核心的部分，花了最多时间在 Prompt 调优上：

1. **配方生成 Prompt**：
   - 尝试1：直接让 LLM 生成配方 → 经常出现不存在的原料
   - 尝试2：明确列出 9 种原料 → 成功率大幅提升
   - 尝试3：增加拒绝逻辑和容量控制 → 完美

2. **视觉识别 Prompt**：
   - 尝试1：直接让 VLM "找出所有物体" → 识别不准
   - 尝试2：提供颜色特征库 → 准确率显著提升
   - 尝试3：强调行列索引规则 → 坐标输出正确

3. **动作规划 Prompt**：
   - 尝试1：让 LLM 自由发挥 → 生成的动作不合理
   - 尝试2：提供 14 步 SOP → 动作序列稳定可靠

---

### 第四步：实现机械臂控制

创建 `robot_controller.py`，重点是 IK 计算：
```python
p.calculateInverseKinematics(
    robotId, endEffectorIndex, targetPos, targetOrn,
    lowerLimits=..., upperLimits=..., restPoses=...
)
```

**技巧**：
- 加入零空间约束后，机械臂不再"抖动"
- 平滑插值让动作更自然

---

### 第五步：串联所有模块

创建 `agent.py`，按照流程依次调用各模块：
1. 初始化所有子系统
2. 接收用户订单
3. 调用 RecipeLLM 生成配方
4. 调用 CameraManager 拍照
5. 调用 VisionLLM 识别位置
6. 核对库存
7. 调用 End2EndPlanner 生成动作
8. 调用 RobotController 执行动作

---

## 技术特点

1. **完全模块化**：每个模块可以独立运行和测试
2. **零硬件成本**：纯软件仿真，不需要真实机器人
3. **易于扩展**：
   - 增加新原料？修改 `coffee_env.py` 的原料列表
   - 更换机械臂？修改 `robot_controller.py` 的 URDF 路径
   - 换个任务场景？重新设计 Prompt
4. **教学友好**：代码简洁，每个函数职责单一

---

## 心得体会

### 关于 Prompt Engineering

Prompt 设计是整个项目的灵魂，我学到了几个关键点：
1. **明确约束**：不要让 LLM "自由发挥"，要给它清晰的边界
2. **提供示例**：JSON 输出格式、坐标计算公式，都要有 demo
3. **迭代优化**：没有一次成功的 Prompt，都是试出来的

### 关于具身智能

我们常见的具身智能需要：
- 大量的演示数据（模仿学习）
- 或者端到端的视觉-动作映射（VLA）

但这个项目展示了另一种可能性：**用 LLM/VLM 作为"规划器"，用传统机器人控制作为"执行器"**。这种方案的优势是：
- 不需要训练数据
- 逻辑清晰，可解释性强
- 易于调试和修改

---

## 总结

这个项目从构思到完成，实际编码时间不到 2 小时，但它展示了：
- 如何用 LLM/VLM 做具身智能
- 如何在没有框架的情况下快速搭建原型
- 如何设计模块化的代码架构

希望这个案例能帮助更多本科生入门具身智能，感受 AI 与机器人结合的魅力！

---

## 附录：快速复现指南

详细的环境配置和使用说明请参考 [README.md](README.md)。

关键步骤：
1. 安装 Python 3.10 + PyBullet
2. 获取智谱 AI API 密钥
3. 启动 `coffee_env.py`（仿真环境）
4. 启动 `agent.py`（主程序）
5. 输入订单，看机械臂制作咖啡！

---

**项目地址**：https://github.com/231055558/robot_vlm_routine
